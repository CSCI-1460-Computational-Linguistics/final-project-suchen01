{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b0MLqDYLdLHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e41c72-f09f-4120-df08-a7663e2d157a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"/content/drive/MyDrive/CSCI 1460, NLP/final project/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hXiL6mlFmssL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a6f998-7b84-462a-ede2-97af8a9f2486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "\n",
        "# https://stackoverflow.com/questions/31126596/saving-response-from-requests-to-file\n",
        "response = requests.get('http://dong.li/lang2logic/seq2seq_jobqueries.zip')\n",
        "if response.status_code == 200:\n",
        "  # https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
        "  with zipfile.ZipFile(io.BytesIO(response.content), \"r\") as zip_ref:\n",
        "    zip_ref.extractall(FILEPATH)\n",
        "  print(\"Extraction completed.\")\n",
        "else:\n",
        "  print(\"Failed to download the zip file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "  \"\"\"Defines a Dataset object for the Jobs dataset to be used with Dataloader\"\"\"\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "# def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "#   \"\"\"\n",
        "#   Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "#   Parameters\n",
        "#   ----------\n",
        "#   batch : list[tuple[list[int], list[int]]]\n",
        "#       a list of outputs of __getitem__\n",
        "\n",
        "#   Returns\n",
        "#   ----------\n",
        "#   tuple[torch.Tensor, torch.Tensor]\n",
        "#       a batched set of input sequences and a batched set of target sequences\n",
        "#   \"\"\"\n",
        "#   src, tgt = default_collate(batch)\n",
        "#   return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def collate(batch: list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch : list[tuple[list[int], list[int]]]\n",
        "        a list of outputs of __getitem__\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[torch.Tensor, torch.Tensor]\n",
        "        a batched set of input sequences and a batched set of target sequences\n",
        "    \"\"\"\n",
        "    src, tgt = zip(*batch)  # Unzips the batch into src and tgt lists\n",
        "    src = torch.tensor(src, dtype=torch.long)  # Convert src to a tensor\n",
        "    tgt = torch.tensor(tgt, dtype=torch.long)  # Convert tgt to a tensor\n",
        "    return src, tgt\n",
        "\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# TODO: Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUERY_VOCAB_LEN = len(query_vocab)\n",
        "# LF_VOCAB_LEN = len(lf_vocab)\n",
        "\n",
        "# def create_model():\n",
        "#   \"\"\"\n",
        "#   Returns your model!\n",
        "\n",
        "#   Returns\n",
        "#   ----------\n",
        "#   ???\n",
        "#       your model!\n",
        "#   \"\"\"\n",
        "#   pass\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# the above is the stencil code and below is my implementation\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# initialize hyperparameters randomly:\n",
        "EMBED_SIZE = 128 # Dimensionality of word embeddings\n",
        "HIDDEN_SIZE = 256   # Number of units in the LSTM hidden layers\n",
        "NUM_LAYERS = 2 # Number of LSTM layers, or number of encoders/decoders\n",
        "\n",
        "# Given vocab sizes from the pre-processing stencil code\n",
        "QUERY_VOCAB_LEN = len(query_vocab)\n",
        "LF_VOCAB_LEN = len(lf_vocab)\n",
        "\n",
        "# Special indices (already extracted)\n",
        "LF_SOS_INDEX = lf_word2idx['<s>'] # Start-of-sequence token index for logical form\n",
        "LF_EOS_INDEX = lf_word2idx['</s>'] # End-of-sequence token index for logical form\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']  # Padding token index for logical form\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for the sequence-to-sequence model, implemented with multiple LSTM layers.\n",
        "    Responsible for encoding the input sequence into a fixed-size context representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, embed_size, hidden_size, pad_idx, num_layers=3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embed_size,\n",
        "                            hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=False)\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "        src: Tensor of shape (batch, seq_len) representing input sequences.\n",
        "        return: LSTM outputs and hidden/cell states.\n",
        "        \"\"\"\n",
        "        # Convert input tokens to dense vectors\n",
        "        embedded = self.embedding(src)\n",
        "        # outputs: Hidden states for all timesteps\n",
        "        # h, c: Final hidden and cell states from the top LSTM layer\n",
        "        outputs, (h, c) = self.lstm(embedded)\n",
        "\n",
        "        return outputs, (h, c)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism for computing attention weights and context vectors.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden_top):\n",
        "        \"\"\"\n",
        "        Compute the attention context vector and weights.\n",
        "        encoder_outputs: Tensor of shape (batch, src_len, hidden_size), encoder outputs for all timesteps.\n",
        "        decoder_hidden_top: Tensor of shape (batch, hidden_size), decoder's top-layer hidden state.\n",
        "        return: Context vector and attention weights.\n",
        "        \"\"\"\n",
        "        # encoder_outputs: (batch, src_len, hidden_size)\n",
        "        # decoder_hidden_top: (batch, hidden_size) - top layer hidden state of decoder\n",
        "\n",
        "        # Add a dimension for batch matrix multiplication (batch, 1, hidden_size)\n",
        "        decoder_hidden_top = decoder_hidden_top.unsqueeze(1)\n",
        "        # Compute raw attention scores\n",
        "        attention_scores = torch.bmm(encoder_outputs, decoder_hidden_top.transpose(1,2)).squeeze(-1)\n",
        "\n",
        "        # Softmax and normalize the socres into probabilities\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "\n",
        "        # Compute context vector as weighted sum of encoder_outputs\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder with multiple LSTM layers and attention mechanism.\n",
        "    Generates the output sequence token-by-token.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size, embed_size, hidden_size, pad_idx, num_layers=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embed_size,\n",
        "                            hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "        # Attention mechanism to focus on relevant encoder outputs\n",
        "        self.attention = Attention()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size) # Fully connected layer to transform the decoder hidden state\n",
        "        self.W2 = nn.Linear(hidden_size, hidden_size) # Fully connected layer to transform the attention context vector\n",
        "        self.Wo = nn.Linear(hidden_size, output_size) # Output projection layer for generating token probabilities\n",
        "        self.tanh = nn.Tanh()  # Activation function\n",
        "        self.num_layers = num_layers # Number of LSTM layers in the decoder\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "        input_token: Tensor of shape (batch), input token indices at the current timestep.\n",
        "        hidden: Tensor of shape (num_layers, batch, hidden_size), decoder hidden states from the previous step.\n",
        "        Tensor of shape (num_layers, batch, hidden_size), decoder cell states from the previous step.\n",
        "        ncoder_outputs: Tensor of shape (batch, src_len, hidden_size), encoder outputs for the entire input sequence.\n",
        "        return: Log probabilities of the next token, updated hidden and cell states, and attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        # Embed input token\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)\n",
        "\n",
        "        # Run one step of LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # output: (batch, 1, hidden_size)\n",
        "        # hidden, cell: (num_layers, batch, hidden_size)\n",
        "\n",
        "        # Use the top layer of hidden state for attention\n",
        "        top_layer_hidden = hidden[-1]  # (batch, hidden_size)\n",
        "        # Compute attention context vector and weights\n",
        "        context, attn_weights = self.attention(encoder_outputs, top_layer_hidden)\n",
        "\n",
        "        # h_t^{att} = tanh(W1 h_t^{L} + W2 c^t)\n",
        "        # Compute attention-modified hidden state\n",
        "        h_att = self.tanh(self.W1(top_layer_hidden) + self.W2(context)) # (batch, hidden_size)\n",
        "\n",
        "        # Project to output vocabulary size\n",
        "        logits = self.Wo(h_att)\n",
        "\n",
        "        # Convert logits to log probabilities\n",
        "        # used log_softmax for torch.nn.NLLLoss when evaluating the model's performance,\n",
        "        # but this loss, torch.nn.NLLLoss, is not printed in the final submission\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return log_probs, hidden, cell, attn_weights\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Sequence-to-sequence model that stacks together the encoder, decoder, and attention mechanism.\n",
        "    Handles the end-to-end transformation of input sequences to output sequences.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size, output_size,\n",
        "                 embed_size, hidden_size,\n",
        "                 src_pad_idx, trg_pad_idx,\n",
        "                 num_layers=3):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_size, embed_size, hidden_size, src_pad_idx, num_layers=num_layers)\n",
        "        self.decoder = Decoder(output_size, embed_size, hidden_size, trg_pad_idx, num_layers=num_layers)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src, trg=None, teacher_forcing=True):\n",
        "        \"\"\"\n",
        "        Forward pass for the sequence-to-sequence model.\n",
        "        src: Tensor of shape (batch, src_len), input sequences.\n",
        "        trg: Tensor of shape (batch, trg_len), target sequences for teacher forcing.\n",
        "        teacher_forcing: Boolean, whether to use teacher forcing during training.\n",
        "        Log probabilities of output sequence tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a target is provided, extract its sequence length.\n",
        "        # if not, raise an error because we cannot use teacher forcing when the target is not provided\n",
        "        # just a sanity check during my debugging process\n",
        "        batch_size, src_len = src.shape\n",
        "        if trg is not None:\n",
        "            trg_len = trg.shape[1]\n",
        "        else:\n",
        "            teacher_forcing = False\n",
        "            raise ValueError(\"No target provided for forward pass in training mode.\")\n",
        "\n",
        "        # # Encode the input sequence\n",
        "        encoder_outputs, (h, c) = self.encoder(src)\n",
        "\n",
        "        # Initialize decoder hidden and cell\n",
        "        dec_hidden = h\n",
        "        dec_cell = c\n",
        "\n",
        "        # The first input token to the decoder is <s>, the start token\n",
        "        input_token = trg[:,0]\n",
        "\n",
        "        # Collect log probabilities for all timesteps\n",
        "        log_probs_seq = []\n",
        "        for t in range(1, trg_len):\n",
        "            # Decode one step\n",
        "            log_probs, dec_hidden, dec_cell, attn_weights = self.decoder(input_token, dec_hidden, dec_cell, encoder_outputs)\n",
        "            log_probs_seq.append(log_probs.unsqueeze(1)) # Append log probabilities for the current timestep\n",
        "\n",
        "            # Next input token\n",
        "            if teacher_forcing:\n",
        "                input_token = trg[:,t] # Use the next token from the ground truth\n",
        "            else:\n",
        "                top1 = log_probs.argmax(1) # Use the token with the highest probability as the next input\n",
        "                input_token = top1\n",
        "\n",
        "        # Combine log probabilities for all timesteps (batch, trg_len-1, output_size)\n",
        "        log_probs_seq = torch.cat(log_probs_seq, dim=1) # (batch, trg_len-1, output_size)\n",
        "        return log_probs_seq\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"\n",
        "    Create and return an instance of the Seq2Seq model.\n",
        "    return: A Seq2Seq model ready for training and testing.\n",
        "    \"\"\"\n",
        "    model = Seq2Seq(\n",
        "        input_size=QUERY_VOCAB_LEN,\n",
        "        output_size=LF_VOCAB_LEN,\n",
        "        embed_size=EMBED_SIZE,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        src_pad_idx=query_word2idx['<PAD>'],\n",
        "        trg_pad_idx=LF_PAD_INDEX,\n",
        "        num_layers=3\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JLQJNO_vms1f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# TODO: Training and testing loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UT5eiZM0AnTf"
      },
      "outputs": [],
      "source": [
        "# def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int=5,\n",
        "#           device: str=\"cuda\") -> nn.Module:\n",
        "#   \"\"\"\n",
        "#   Trains your model!\n",
        "\n",
        "#   Parameters\n",
        "#   ----------\n",
        "#   model : nn.Module\n",
        "#       your model!\n",
        "#   train_dataloader : DataLoader\n",
        "#       a dataloader of the training data from build_dataloaders\n",
        "#   num_epochs : int\n",
        "#       number of epochs to train for\n",
        "#   device : str\n",
        "#       device that the model is running on\n",
        "\n",
        "#   Returns\n",
        "#   ----------\n",
        "#   ???\n",
        "#       your trained model\n",
        "#   \"\"\"\n",
        "#   pass\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# the above is the stencil code and below is my implementation\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "def train(model: nn.Module, train_dataloader: DataLoader, num_epochs: int=5,\n",
        "          device: str=\"cuda\") -> nn.Module:\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.NLLLoss(ignore_index=LF_PAD_INDEX, reduction='mean')\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Iterate through each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0 # Accumulator to track the total loss for the epoch\n",
        "\n",
        "        # Loop through each batch in the training DataLoader.\n",
        "        for batch_idx, (src, trg) in enumerate(train_dataloader):\n",
        "            # Move source (src) and target (trg) tensors to the specified device.\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            optimizer.zero_grad() # Zero the gradients from the previous batch to prevent accumulation.\n",
        "\n",
        "            # Perform a forward pass with the model.\n",
        "            # Teacher forcing is used during training (trg is provided).\n",
        "            # It represents log probabilities for the output vocabulary at each timestep.\n",
        "            log_probs_seq = model(src, trg, teacher_forcing=True)\n",
        "\n",
        "            # Align the target sequence for loss calculation.\n",
        "            # trg[:, 1:] represents the target sequence shifted by one (ignoring <s> at the start).\n",
        "            # Reshape both log_probs_seq and trg[:, 1:] to match for loss computation.\n",
        "            loss = criterion(\n",
        "                log_probs_seq.reshape(-1, LF_VOCAB_LEN),\n",
        "                trg[:,1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()  # Backpropagate the loss to compute gradients.\n",
        "            optimizer.step() # Update the model parameters using the computed gradients.\n",
        "\n",
        "            # Add the batch loss to the total loss for this epoch.\n",
        "            total_loss += loss.item()\n",
        "        # Calculate the average loss for the epoch.\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "# def evaluate(model: nn.Module, dataloader: DataLoader, device: str=\"cuda\") -> tuple[int, int]:\n",
        "#   \"\"\"\n",
        "#   Evaluates your model!\n",
        "\n",
        "#   Parameters\n",
        "#   ----------\n",
        "#   model : nn.Module\n",
        "#       your model!\n",
        "#   dataloader : DataLoader\n",
        "#       a dataloader of the testing data from build_dataloaders\n",
        "#   device : str\n",
        "#       device that the model is running on\n",
        "\n",
        "#   Returns\n",
        "#   ----------\n",
        "#   tuple[int, int]\n",
        "#       per-token accuracy and exact_match accuracy\n",
        "#   \"\"\"\n",
        "#   pass\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# the above is the stencil code and below is my implementation\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str=\"cuda\") -> tuple[float, float]:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize counters for evaluation metrics\n",
        "    total_tokens = 0 # Total number of valid tokens in the dataset (excluding padding)\n",
        "    correct_tokens = 0 # Count of correctly predicted tokens\n",
        "    exact_matches = 0 # Count of sequences that match the target exactly\n",
        "\n",
        "    # Disable gradient computation to save memory and computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop through each batch in the dataloader\n",
        "        for src, trg in dataloader:\n",
        "            # src: (1, src_len) - Input sequence\n",
        "            # trg: (1, trg_len) - Target sequence\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            batch_size, trg_len = trg.shape # Ensure that each batch contains only one sample (batch size of 1)\n",
        "            assert batch_size == 1, \"Test batch size should be 1\"\n",
        "\n",
        "            # Encode the input sequence using the encoder\n",
        "            encoder_outputs, (h, c) = model.encoder(src)\n",
        "            # encoder_outputs: Hidden states from the encoder (batch, src_len, hidden_size)\n",
        "            # h, c: Final hidden and cell states from the encoder (num_layers, batch, hidden_size)\n",
        "            dec_hidden = h\n",
        "            dec_cell = c\n",
        "\n",
        "            # Start decoding\n",
        "            input_token = trg[:,0] # <s>\n",
        "            generated = [input_token.item()]\n",
        "            # Decode one step at a time for trg_len - 1 steps (excluding the start token)\n",
        "            for t in range(1, trg_len):\n",
        "                log_probs, dec_hidden, dec_cell, _ = model.decoder(input_token, dec_hidden, dec_cell, encoder_outputs)\n",
        "                # Select the token with the highest probability (greedy decoding)\n",
        "                top1 = log_probs.argmax(1)\n",
        "                generated.append(top1.item())\n",
        "                # Move to the next input token for the decoder\n",
        "                input_token = top1\n",
        "\n",
        "                # Termination condition:\n",
        "                # The code is set up to generate the full sequence, even if an <EOS> token is encountered.\n",
        "\n",
        "\n",
        "            # Post-decoding: Evaluate the generated sequence\n",
        "            # Ignore the <s> token when comparing with the target sequence\n",
        "            pred_seq = generated[1:]  # ignoring the <s> we started with\n",
        "            gold_seq = trg[0,1:].tolist() # ignore the first token <s>\n",
        "\n",
        "            # Compute per-token accuracy\n",
        "            for p, g in zip(pred_seq, gold_seq):\n",
        "                if g != LF_PAD_INDEX: # don't count padding\n",
        "                    total_tokens += 1\n",
        "                    if p == g:\n",
        "                        correct_tokens += 1\n",
        "\n",
        "            # Compute exact match accuracy\n",
        "            # Remove padding tokens from the target sequence\n",
        "            gold_no_pad = [x for x in gold_seq if x != LF_PAD_INDEX]\n",
        "            # Trim the predicted sequence to the length of the non-padded target sequence\n",
        "            pred_no_pad = pred_seq[:len(gold_no_pad)]\n",
        "            if pred_no_pad == gold_no_pad:\n",
        "                exact_matches += 1\n",
        "\n",
        "    per_token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
        "    exact_match_accuracy = exact_matches / len(dataloader)\n",
        "    return per_token_accuracy, exact_match_accuracy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0qSnLCPeiI1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e8d16d-f07a-44e1-d9fc-0de0417fa0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.7042\n",
            "Epoch 2/20, Loss: 2.0531\n",
            "Epoch 3/20, Loss: 1.1222\n",
            "Epoch 4/20, Loss: 0.8024\n",
            "Epoch 5/20, Loss: 0.6956\n",
            "Epoch 6/20, Loss: 0.6201\n",
            "Epoch 7/20, Loss: 0.5484\n",
            "Epoch 8/20, Loss: 0.4807\n",
            "Epoch 9/20, Loss: 0.4186\n",
            "Epoch 10/20, Loss: 0.3559\n",
            "Epoch 11/20, Loss: 0.3066\n",
            "Epoch 12/20, Loss: 0.2609\n",
            "Epoch 13/20, Loss: 0.2121\n",
            "Epoch 14/20, Loss: 0.1859\n",
            "Epoch 15/20, Loss: 0.1666\n",
            "Epoch 16/20, Loss: 0.1392\n",
            "Epoch 17/20, Loss: 0.1118\n",
            "Epoch 18/20, Loss: 0.0982\n",
            "Epoch 19/20, Loss: 0.0852\n",
            "Epoch 20/20, Loss: 0.0770\n",
            "Test Per-token Accuracy: 0.8757281553398059\n",
            "Test Exact-match Accuracy: 0.7428571428571429\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    jobs_train, jobs_test = build_datasets()\n",
        "    dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=20)\n",
        "    model = create_model()\n",
        "    model = train(model, dataloader_train, num_epochs=20, device=device)\n",
        "    test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "    print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "\n",
        "    # # Adjusted to unpack three values\n",
        "    # test_per_token_accuracy, test_exact_match_accuracy, test_loss = evaluate(model, dataloader_test, device=device)\n",
        "\n",
        "    # print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    # print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "    # print(f'Test Loss: {test_loss:.4f}')  # Optionally print the loss\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_hfJFfYRSFBV",
        "RCKjb4HsMKw-"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}